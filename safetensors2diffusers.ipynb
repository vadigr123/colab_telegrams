{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0oo0oNaztRsDDDM+gqTTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadigr123/colab_telegrams/blob/main/safetensors2diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing\n",
        "!apt -y install -qq aria2\n",
        "!pip install -q diffusers transformers accelerate safetensors huggingface_hub omegaconf"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eyd-dl3znacF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Checkpoints (SDXL/SD Models) - Standalone { display-mode: \"form\" }\n",
        "#@markdown Download SDXL and SD 1.5 checkpoint models from various sources\n",
        "\n",
        "import os\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "%cd /content\n",
        "\n",
        "def download_things(directory, url, hf_token=\"\", civitai_api_key=\"\"):\n",
        "    \"\"\"Download function for various sources\"\"\"\n",
        "    url = url.strip()\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        original_dir = os.getcwd()\n",
        "        os.chdir(directory)\n",
        "        !gdown --fuzzy {url}\n",
        "        os.chdir(original_dir)\n",
        "    elif \"huggingface.co\" in url:\n",
        "        url = url.replace(\"?download=true\", \"\")\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 {url} -d {directory}  -o {url.split('/')[-1]}\n",
        "    elif \"civitai.com\" in url:\n",
        "        if civitai_api_key:\n",
        "            # Add the API key as a parameter\n",
        "            separator = \"&\" if \"?\" in url else \"?\"\n",
        "            final_url = f\"{url}{separator}token={civitai_api_key}\"\n",
        "            !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {directory} \"{final_url}\"\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è You need an API key to download Civitai models.\")\n",
        "            return False\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {directory} {url}\n",
        "\n",
        "    return True\n",
        "\n",
        "# Create models directory\n",
        "directory_models = 'models'\n",
        "os.makedirs(directory_models, exist_ok=True)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Model URLs** (comma-separated): Civitai API, Google Drive, or Hugging Face links\n",
        "checkpoint_urls = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **API Keys** (if needed)\n",
        "#@markdown Civitai API Key (required for Civitai downloads) - Get from https://civitai.com/user/account\n",
        "civitai_key = \"\"  #@param {type:\"string\"}\n",
        "#@markdown Hugging Face Token (for private repos)\n",
        "hf_token = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "def get_existing_models():\n",
        "    \"\"\"Get list of existing model files\"\"\"\n",
        "    valid_extensions = {'.ckpt', '.pt', '.pth', '.safetensors', '.bin'}\n",
        "    existing_files = []\n",
        "\n",
        "    if os.path.exists(directory_models):\n",
        "        for filename in os.listdir(directory_models):\n",
        "            if any(filename.endswith(ext) for ext in valid_extensions):\n",
        "                existing_files.append(filename)\n",
        "\n",
        "    return existing_files\n",
        "\n",
        "def download_checkpoints(urls_string, civitai_api_key=\"\", hf_token=\"\"):\n",
        "    \"\"\"Download checkpoint models from provided URLs\"\"\"\n",
        "    if not urls_string.strip():\n",
        "        print(\"‚ö†Ô∏è No URLs provided!\")\n",
        "        return\n",
        "\n",
        "    urls = [url.strip() for url in urls_string.split(',') if url.strip()]\n",
        "\n",
        "    if not urls:\n",
        "        print(\"‚ö†Ô∏è No valid URLs found!\")\n",
        "        return\n",
        "\n",
        "    # Check for Civitai URLs without API key\n",
        "    civitai_urls = [url for url in urls if \"civitai.com\" in url]\n",
        "    if civitai_urls and not civitai_api_key.strip():\n",
        "        print(\"üîë Civitai API Key Required!\")\n",
        "        print(\"   You have Civitai URLs but no API key provided.\")\n",
        "        print(\"   Get your API key from: https://civitai.com/user/account\")\n",
        "        print(\"   Then add it to the 'civitai_key' field above.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üì• Starting download of {len(urls)} model(s)...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    existing_before = get_existing_models()\n",
        "    downloaded_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        print(f\"üîÑ [{i}/{len(urls)}] Processing: {url}\")\n",
        "\n",
        "        # For debugging, show the final URL for Civitai\n",
        "        if \"civitai.com\" in url and civitai_api_key:\n",
        "            separator = \"&\" if \"?\" in url else \"?\"\n",
        "            final_url = f\"{url}{separator}token={civitai_api_key}\"\n",
        "            print(f\"   üîó Final URL: {final_url[:80]}...\")\n",
        "\n",
        "        try:\n",
        "            success = download_things(directory_models, url, hf_token, civitai_api_key)\n",
        "            if success is False:\n",
        "                failed_count += 1\n",
        "                print(f\"   ‚ùå Download failed!\")\n",
        "            else:\n",
        "                downloaded_count += 1\n",
        "                print(f\"   ‚úÖ Download completed!\")\n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            print(f\"   ‚ùå Error: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Check what was actually downloaded\n",
        "    existing_after = get_existing_models()\n",
        "    new_files = set(existing_after) - set(existing_before)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìä Download Summary:\")\n",
        "    print(f\"   ‚úÖ Successful downloads: {downloaded_count}\")\n",
        "    print(f\"   ‚ùå Failed downloads: {failed_count}\")\n",
        "    print(f\"   üìÅ New files added: {len(new_files)}\")\n",
        "\n",
        "    if new_files:\n",
        "        print(f\"\\nüÜï New model files:\")\n",
        "        for file in sorted(new_files):\n",
        "            file_path = os.path.join(directory_models, file)\n",
        "            if os.path.exists(file_path):\n",
        "                file_size = os.path.getsize(file_path) / (1024**3)  # Size in GB\n",
        "                print(f\"   üìÑ {file} ({file_size:.2f} GB)\")\n",
        "\n",
        "# Execute the download\n",
        "print(\"üöÄ Checkpoint Downloader Starting...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Verify API key is present\n",
        "if civitai_key.strip():\n",
        "    print(f\"üîë Civitai API Key: {civitai_key[:10]}...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No Civitai API Key provided\")\n",
        "\n",
        "try:\n",
        "    download_checkpoints(checkpoint_urls, civitai_key, hf_token)\n",
        "\n",
        "    # List all available models\n",
        "    print(\"\\nüóÇÔ∏è All Available Models:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    all_models = get_existing_models()\n",
        "\n",
        "    if all_models:\n",
        "        total_size = 0\n",
        "        for filename in sorted(all_models):\n",
        "            file_path = os.path.join(directory_models, filename)\n",
        "            if os.path.exists(file_path):\n",
        "                file_size = os.path.getsize(file_path) / (1024**3)  # Size in GB\n",
        "                total_size += file_size\n",
        "                print(f\"üìÑ {filename} ({file_size:.2f} GB)\")\n",
        "\n",
        "        print(f\"\\nüíæ Total storage used: {total_size:.2f} GB\")\n",
        "    else:\n",
        "        print(\"   No models found in the models directory.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Unexpected error: {str(e)}\")\n",
        "\n",
        "print(\"\\nüéâ Checkpoint download process completed!\")"
      ],
      "metadata": {
        "id": "Jkyjb8TqmAue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AS2IPhH5jBGF"
      },
      "outputs": [],
      "source": [
        "# SafeTensor to Diffusers Converter for Google Colab\n",
        "# This script converts SafeTensor checkpoint models to Diffusers format and uploads to Hugging Face\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate safetensors huggingface_hub omegaconf\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "from huggingface_hub import HfApi, create_repo, upload_folder\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "import shutil\n",
        "import tempfile\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "class SafeTensorToDiffusersConverter:\n",
        "    def __init__(self):\n",
        "        self.supported_base_models = {\n",
        "            'sd1.5': 'runwayml/stable-diffusion-v1-5',\n",
        "            'sd2.1': 'stabilityai/stable-diffusion-2-1',\n",
        "            'sdxl': 'stabilityai/stable-diffusion-xl-base-1.0',\n",
        "            'sd1.4': 'CompVis/stable-diffusion-v1-4',\n",
        "            'sd2.0': 'stabilityai/stable-diffusion-2',\n",
        "            'pony': 'AstraliteHeart/pony-diffusion-v6-xl',\n",
        "            'noobai': 'Laxhar/noobai-XL-1.0',\n",
        "            'pony_v6': 'AstraliteHeart/pony-diffusion-v6-xl',\n",
        "            'noobai_xl': 'Laxhar/noobai-XL-1.0',\n",
        "        }\n",
        "\n",
        "    def detect_base_model(self, checkpoint_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Detect the base model from SafeTensor checkpoint\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load the safetensor file\n",
        "            state_dict = load_file(checkpoint_path, device=\"cpu\")\n",
        "\n",
        "            # Check for specific model signatures\n",
        "            filename = os.path.basename(checkpoint_path).lower()\n",
        "\n",
        "            # Check filename for model type hints\n",
        "            if any(term in filename for term in ['pony', 'pdxl', 'ponyxl']):\n",
        "                return self.supported_base_models['pony']\n",
        "            elif any(term in filename for term in ['noob', 'nai', 'noobai']):\n",
        "                return self.supported_base_models['noobai']\n",
        "\n",
        "            # Check model dimensions and architecture to determine base model\n",
        "            # Look for specific keys that indicate model type\n",
        "            if \"model.diffusion_model.input_blocks.0.0.weight\" in state_dict:\n",
        "                input_channels = state_dict[\"model.diffusion_model.input_blocks.0.0.weight\"].shape[1]\n",
        "\n",
        "                # Check UNet architecture\n",
        "                if \"model.diffusion_model.middle_block.1.proj_in.weight\" in state_dict:\n",
        "                    middle_dim = state_dict[\"model.diffusion_model.middle_block.1.proj_in.weight\"].shape[0]\n",
        "                    if middle_dim == 2048:  # SDXL-based (could be Pony or NoobAI)\n",
        "                        # Additional checks for Pony/NoobAI specific features\n",
        "                        if self._is_pony_model(state_dict):\n",
        "                            return self.supported_base_models['pony']\n",
        "                        elif self._is_noobai_model(state_dict):\n",
        "                            return self.supported_base_models['noobai']\n",
        "                        else:\n",
        "                            return self.supported_base_models['sdxl']\n",
        "                    elif middle_dim == 1024:  # SD 2.x\n",
        "                        return self.supported_base_models['sd2.1']\n",
        "                    else:  # SD 1.x\n",
        "                        return self.supported_base_models['sd1.5']\n",
        "\n",
        "            # Fallback: check file size as rough indicator\n",
        "            file_size = os.path.getsize(checkpoint_path) / (1024**3)  # GB\n",
        "            if file_size > 5:  # SDXL-based models are typically larger\n",
        "                return self.supported_base_models['sdxl']\n",
        "            elif file_size > 3:  # SD 2.x models\n",
        "                return self.supported_base_models['sd2.1']\n",
        "            else:  # SD 1.x models\n",
        "                return self.supported_base_models['sd1.5']\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error detecting base model: {e}\")\n",
        "            print(\"Defaulting to Stable Diffusion XL\")\n",
        "            return self.supported_base_models['sdxl']\n",
        "\n",
        "    def _is_pony_model(self, state_dict: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Check if the model has Pony-specific characteristics\"\"\"\n",
        "        # Pony models often have specific layer configurations\n",
        "        # This is a heuristic check - you might need to adjust based on actual Pony model structure\n",
        "        pony_indicators = [\n",
        "            \"model.diffusion_model.label_emb\",  # Pony often has label embedding\n",
        "            \"model.diffusion_model.time_embed.2\",  # Specific time embedding structure\n",
        "        ]\n",
        "        return any(key in state_dict for key in pony_indicators)\n",
        "\n",
        "    def _is_noobai_model(self, state_dict: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Check if the model has NoobAI-specific characteristics\"\"\"\n",
        "        # NoobAI models might have specific architectural features\n",
        "        # This is a heuristic check - adjust based on actual NoobAI model structure\n",
        "        noobai_indicators = [\n",
        "            # Add specific NoobAI model indicators here\n",
        "            \"model.diffusion_model.zero_convs\",  # Example indicator\n",
        "        ]\n",
        "        return any(key in state_dict for key in noobai_indicators)\n",
        "\n",
        "    def convert_safetensor_to_diffusers(self, checkpoint_path: str, output_dir: str, base_model: str = None) -> str:\n",
        "        \"\"\"\n",
        "        Convert SafeTensor checkpoint to Diffusers format\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Detect base model if not provided\n",
        "            if base_model is None:\n",
        "                base_model = self.detect_base_model(checkpoint_path)\n",
        "                print(f\"Detected base model: {base_model}\")\n",
        "\n",
        "            # Create temporary directory for conversion\n",
        "            temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "            # Load the base pipeline\n",
        "            print(f\"Loading base pipeline from {base_model}...\")\n",
        "            if \"xl\" in base_model.lower():\n",
        "                from diffusers import StableDiffusionXLPipeline\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "                    base_model,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True\n",
        "                )\n",
        "            else:\n",
        "                pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                    base_model,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True\n",
        "                )\n",
        "\n",
        "            # Load checkpoint weights\n",
        "            print(\"Loading checkpoint weights...\")\n",
        "            checkpoint = load_file(checkpoint_path, device=\"cpu\")\n",
        "\n",
        "            # Convert checkpoint format to diffusers format\n",
        "            converted_state_dict = self._convert_checkpoint_to_diffusers(checkpoint, pipeline)\n",
        "\n",
        "            # Load converted weights into pipeline\n",
        "            if converted_state_dict:\n",
        "                pipeline.unet.load_state_dict(converted_state_dict.get('unet', {}), strict=False)\n",
        "                if 'vae' in converted_state_dict:\n",
        "                    pipeline.vae.load_state_dict(converted_state_dict['vae'], strict=False)\n",
        "                if 'text_encoder' in converted_state_dict:\n",
        "                    pipeline.text_encoder.load_state_dict(converted_state_dict['text_encoder'], strict=False)\n",
        "\n",
        "            # Save the converted pipeline\n",
        "            print(f\"Saving converted pipeline to {output_dir}...\")\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            pipeline.save_pretrained(output_dir, safe_serialization=True)\n",
        "\n",
        "            # Create model card - Fixed method name\n",
        "            self._create_detailed_model_card(output_dir, base_model, checkpoint_path)\n",
        "\n",
        "            print(\"Conversion completed successfully!\")\n",
        "            return output_dir\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during conversion: {e}\")\n",
        "            raise e\n",
        "\n",
        "    def _convert_checkpoint_to_diffusers(self, checkpoint: Dict[str, Any], pipeline) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Convert checkpoint state dict to diffusers format\n",
        "        \"\"\"\n",
        "        converted = {}\n",
        "\n",
        "        # This is a simplified conversion - in practice, you might need more sophisticated mapping\n",
        "        # depending on the specific checkpoint format\n",
        "\n",
        "        unet_state_dict = {}\n",
        "        vae_state_dict = {}\n",
        "        text_encoder_state_dict = {}\n",
        "\n",
        "        for key, value in checkpoint.items():\n",
        "            # Map UNet weights\n",
        "            if key.startswith('model.diffusion_model.'):\n",
        "                new_key = key.replace('model.diffusion_model.', '')\n",
        "                unet_state_dict[new_key] = value\n",
        "\n",
        "            # Map VAE weights\n",
        "            elif key.startswith('first_stage_model.'):\n",
        "                new_key = key.replace('first_stage_model.', '')\n",
        "                vae_state_dict[new_key] = value\n",
        "\n",
        "            # Map Text Encoder weights\n",
        "            elif key.startswith('cond_stage_model.'):\n",
        "                new_key = key.replace('cond_stage_model.', '')\n",
        "                text_encoder_state_dict[new_key] = value\n",
        "\n",
        "        if unet_state_dict:\n",
        "            converted['unet'] = unet_state_dict\n",
        "        if vae_state_dict:\n",
        "            converted['vae'] = vae_state_dict\n",
        "        if text_encoder_state_dict:\n",
        "            converted['text_encoder'] = text_encoder_state_dict\n",
        "\n",
        "        return converted\n",
        "\n",
        "    def _create_detailed_model_card(self, output_dir: str, base_model: str, checkpoint_path: str, model_name: str = \"\", model_description: str = \"\"):\n",
        "        \"\"\"\n",
        "        Create a detailed model card for the converted model\n",
        "        \"\"\"\n",
        "        if not model_name:\n",
        "            model_name = os.path.basename(checkpoint_path).replace('.safetensors', '').replace('.ckpt', '')\n",
        "\n",
        "        # Determine model type for tags\n",
        "        model_type = \"sdxl\" if any(term in base_model.lower() for term in ['xl', 'pony', 'noob']) else \"sd\"\n",
        "\n",
        "        model_card_content = f\"\"\"---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {base_model}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-{model_type}\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- safetensors\n",
        "{\"- pony\" if \"pony\" in base_model.lower() else \"\"}\n",
        "{\"- noobai\" if \"noob\" in base_model.lower() else \"\"}\n",
        "widget:\n",
        "- text: \"a beautiful landscape, masterpiece, best quality\"\n",
        "  example_title: \"Landscape\"\n",
        "- text: \"1girl, anime style, detailed face, high quality\"\n",
        "  example_title: \"Anime Character\"\n",
        "- text: \"a cute cat, photorealistic, 4k\"\n",
        "  example_title: \"Photorealistic\"\n",
        "inference: true\n",
        "---\n",
        "\n",
        "# {model_name}\n",
        "\n",
        "{model_description}\n",
        "\n",
        "## Model Information\n",
        "\n",
        "- **Base Model**: {base_model}\n",
        "- **Original Format**: SafeTensor Checkpoint\n",
        "- **Converted Format**: Diffusers\n",
        "- **Model Type**: {\"Stable Diffusion XL\" if \"xl\" in model_type else \"Stable Diffusion\"}\n",
        "- **Original Checkpoint**: {os.path.basename(checkpoint_path)}\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "```python\n",
        "from diffusers import {\"StableDiffusionXLPipeline\" if model_type == \"sdxl\" else \"StableDiffusionPipeline\"}\n",
        "import torch\n",
        "\n",
        "# Load the pipeline\n",
        "pipeline = {\"StableDiffusionXLPipeline\" if model_type == \"sdxl\" else \"StableDiffusionPipeline\"}.from_pretrained(\n",
        "    \"your-username/your-repo-name\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "\n",
        "# Generate an image\n",
        "prompt = \"a beautiful landscape, masterpiece, best quality\"\n",
        "{\"negative_prompt = 'low quality, blurry, distorted'\" if model_type == \"sdxl\" else \"\"}\n",
        "image = pipeline(\n",
        "    prompt,\n",
        "    {\"negative_prompt=negative_prompt,\" if model_type == \"sdxl\" else \"\"}\n",
        "    num_inference_steps=25,\n",
        "    guidance_scale=7.5,\n",
        "    {\"width=1024, height=1024\" if model_type == \"sdxl\" else \"width=512, height=512\"}\n",
        ").images[0]\n",
        "\n",
        "image.save(\"generated_image.png\")\n",
        "```\n",
        "\n",
        "### Advanced Usage with Custom Settings\n",
        "\n",
        "```python\n",
        "# For more control over generation\n",
        "image = pipeline(\n",
        "    prompt=\"your prompt here\",\n",
        "    {\"negative_prompt='low quality, worst quality',\" if model_type == \"sdxl\" else \"\"}\n",
        "    num_inference_steps=30,\n",
        "    guidance_scale=8.0,\n",
        "    {\"width=1024,\" if model_type == \"sdxl\" else \"width=512,\"}\n",
        "    {\"height=1024,\" if model_type == \"sdxl\" else \"height=512,\"}\n",
        "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
        ").images[0]\n",
        "```\n",
        "\n",
        "## Recommended Settings\n",
        "\n",
        "{\"### For Pony Diffusion Models:\" if \"pony\" in base_model.lower() else \"\"}\n",
        "{\"- **Positive prompts**: Include quality tags like 'score_9, score_8_up, score_7_up'\" if \"pony\" in base_model.lower() else \"\"}\n",
        "{\"- **Negative prompts**: Use 'score_4, score_5, score_6'\" if \"pony\" in base_model.lower() else \"\"}\n",
        "{\"- **CFG Scale**: 6-8 works well\" if \"pony\" in base_model.lower() else \"\"}\n",
        "\n",
        "{\"### For NoobAI Models:\" if \"noob\" in base_model.lower() else \"\"}\n",
        "{\"- **Positive prompts**: Include 'masterpiece, best quality, very aesthetic'\" if \"noob\" in base_model.lower() else \"\"}\n",
        "{\"- **Negative prompts**: Use 'lowres, bad anatomy, bad hands, text, error, missing fingers'\" if \"noob\" in base_model.lower() else \"\"}\n",
        "{\"- **CFG Scale**: 5-7 recommended\" if \"noob\" in base_model.lower() else \"\"}\n",
        "\n",
        "- **Resolution**: {\"1024x1024 or 832x1216 for portraits\" if model_type == \"sdxl\" else \"512x512 or 768x512 recommended\"}\n",
        "- **Steps**: 20-30 steps usually sufficient\n",
        "- **Sampler**: Euler a, DPM++ 2M, or DPM++ SDE work well\n",
        "\n",
        "## Model Details\n",
        "\n",
        "This model was automatically converted from a SafeTensor checkpoint to the Diffusers format for easy use with the ü§ó Diffusers library.\n",
        "\n",
        "### Technical Specifications\n",
        "- **Architecture**: {\"Stable Diffusion XL\" if model_type == \"sdxl\" else \"Stable Diffusion\"}\n",
        "- **Parameter Count**: {\"~3.5B\" if model_type == \"sdxl\" else \"~860M\"}\n",
        "- **Precision**: Mixed precision (FP16/FP32)\n",
        "- **VRAM Requirements**: {\"~6GB\" if model_type == \"sdxl\" else \"~4GB\"} (with FP16)\n",
        "\n",
        "## License\n",
        "\n",
        "This model is licensed under the CreativeML OpenRAIL-M license. Please ensure you comply with the license terms when using this model.\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "This model is converted from a community checkpoint. Please ensure you have the right to use and distribute the original model before using this converted version.\n",
        "\"\"\"\n",
        "\n",
        "        with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
        "            f.write(model_card_content)\n",
        "\n",
        "    def upload_to_huggingface(self, model_dir: str, repo_name: str, token: str, private: bool = False):\n",
        "        \"\"\"\n",
        "        Upload the converted model to Hugging Face\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"Creating repository: {repo_name}\")\n",
        "\n",
        "            # Create repository\n",
        "            api = HfApi(token=token)\n",
        "            create_repo(repo_name, token=token, private=private, exist_ok=True)\n",
        "\n",
        "            print(\"Uploading model to Hugging Face...\")\n",
        "            api.upload_folder(\n",
        "                folder_path=model_dir,\n",
        "                repo_id=repo_name,\n",
        "                token=token,\n",
        "                commit_message=\"Upload converted Stable Diffusion model\"\n",
        "            )\n",
        "\n",
        "            print(f\"Model successfully uploaded to: https://huggingface.co/{repo_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading to Hugging Face: {e}\")\n",
        "            raise e\n",
        "\n",
        "# Main execution function with Colab parameter forms\n",
        "def convert_and_upload_model():\n",
        "    \"\"\"\n",
        "    Main function to convert SafeTensor to Diffusers and upload to Hugging Face\n",
        "    Uses Google Colab parameter forms for easy input\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== GOOGLE COLAB PARAMETER FORMS ==========\n",
        "    print(\"üìã Fill in the parameters below:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Path to SafeTensor checkpoint file\n",
        "    CHECKPOINT_PATH = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Hugging Face Token\n",
        "    HF_TOKEN = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Repository name (format: username/model-name)\n",
        "    REPO_NAME = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Base model selection\n",
        "    BASE_MODEL_TYPE = \"Auto-detect\" # @param [\"Auto-detect\", \"Stable Diffusion 1.5\", \"Stable Diffusion 2.1\", \"Stable Diffusion XL\", \"Pony Diffusion v6 XL\", \"NoobAI XL\", \"Custom\"]\n",
        "\n",
        "    # Custom base model (only used if \"Custom\" is selected above)\n",
        "    CUSTOM_BASE_MODEL = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Repository privacy\n",
        "    PRIVATE_REPO = True # @param {type:\"boolean\"}\n",
        "\n",
        "    # Model name/title for the model card\n",
        "    MODEL_NAME = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Model description\n",
        "    MODEL_DESCRIPTION = \"Converted from SafeTensor checkpoint to Diffusers format!\" # @param {type:\"string\"}\n",
        "\n",
        "    # ================================================\n",
        "\n",
        "    converter = SafeTensorToDiffusersConverter()\n",
        "\n",
        "    # Map base model selection to actual model paths\n",
        "    base_model_mapping = {\n",
        "        \"Auto-detect\": None,\n",
        "        \"Stable Diffusion 1.5\": converter.supported_base_models['sd1.5'],\n",
        "        \"Stable Diffusion 2.1\": converter.supported_base_models['sd2.1'],\n",
        "        \"Stable Diffusion XL\": converter.supported_base_models['sdxl'],\n",
        "        \"Pony Diffusion v6 XL\": converter.supported_base_models['pony'],\n",
        "        \"NoobAI XL\": converter.supported_base_models['noobai'],\n",
        "        \"Custom\": CUSTOM_BASE_MODEL if CUSTOM_BASE_MODEL else None\n",
        "    }\n",
        "\n",
        "    BASE_MODEL = base_model_mapping.get(BASE_MODEL_TYPE)\n",
        "\n",
        "    # Validate inputs\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f\"‚ùå Error: Checkpoint file not found at {CHECKPOINT_PATH}\")\n",
        "        print(\"Please upload your SafeTensor file to Google Colab and update the path.\")\n",
        "        return\n",
        "\n",
        "    if not HF_TOKEN or HF_TOKEN.strip() == \"\":\n",
        "        print(\"‚ùå Error: Please enter your Hugging Face token\")\n",
        "        print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
        "        return\n",
        "\n",
        "    if not REPO_NAME or REPO_NAME.strip() == \"\":\n",
        "        print(\"‚ùå Error: Please enter your repository name (format: username/model-name)\")\n",
        "        return\n",
        "\n",
        "    if \"/\" not in REPO_NAME:\n",
        "        print(\"‚ùå Error: Repository name must be in format 'username/model-name'\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Create output directory\n",
        "        output_dir = \"./converted_model\"\n",
        "\n",
        "        print(\"\\nüöÄ Starting conversion process...\")\n",
        "        print(f\"üìÅ Checkpoint: {CHECKPOINT_PATH}\")\n",
        "        print(f\"üéØ Target repository: {REPO_NAME}\")\n",
        "        print(f\"üîß Base model type: {BASE_MODEL_TYPE}\")\n",
        "        print(f\"üîí Private repository: {PRIVATE_REPO}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Convert SafeTensor to Diffusers\n",
        "        converted_path = converter.convert_safetensor_to_diffusers(\n",
        "            CHECKPOINT_PATH,\n",
        "            output_dir,\n",
        "            BASE_MODEL\n",
        "        )\n",
        "\n",
        "        # Update model card with custom information\n",
        "        if MODEL_NAME or MODEL_DESCRIPTION:\n",
        "            converter._create_detailed_model_card(\n",
        "                output_dir,\n",
        "                BASE_MODEL or \"Auto-detected\",\n",
        "                CHECKPOINT_PATH,\n",
        "                MODEL_NAME,\n",
        "                MODEL_DESCRIPTION\n",
        "            )\n",
        "\n",
        "        # Upload to Hugging Face\n",
        "        converter.upload_to_huggingface(\n",
        "            converted_path,\n",
        "            REPO_NAME,\n",
        "            HF_TOKEN,\n",
        "            PRIVATE_REPO\n",
        "        )\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üéâ CONVERSION COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"üåê Your model is now available at: https://huggingface.co/{REPO_NAME}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Show usage example\n",
        "        print(f\"\\nüìñ Usage Example:\")\n",
        "        print(f\"```python\")\n",
        "        print(f\"from diffusers import StableDiffusionXLPipeline\")\n",
        "        print(f\"import torch\")\n",
        "        print(f\"\")\n",
        "        print(f\"pipeline = StableDiffusionXLPipeline.from_pretrained(\")\n",
        "        print(f\"    '{REPO_NAME}',\")\n",
        "        print(f\"    torch_dtype=torch.float16\")\n",
        "        print(f\")\")\n",
        "        print(f\"pipeline = pipeline.to('cuda')\")\n",
        "        print(f\"\")\n",
        "        print(f\"prompt = 'a beautiful landscape'\")\n",
        "        print(f\"image = pipeline(prompt).images[0]\")\n",
        "        print(f\"image.save('generated_image.png')\")\n",
        "        print(f\"```\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during process: {e}\")\n",
        "        print(\"Please check your inputs and try again.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Example usage with step-by-step instructions\n",
        "def show_usage_example():\n",
        "    \"\"\"\n",
        "    Show usage example and instructions\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üé® SAFETENSOR TO DIFFUSERS CONVERTER\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(\"üìã SUPPORTED BASE MODELS:\")\n",
        "    print(\"‚Ä¢ Stable Diffusion 1.4/1.5\")\n",
        "    print(\"‚Ä¢ Stable Diffusion 2.0/2.1\")\n",
        "    print(\"‚Ä¢ Stable Diffusion XL\")\n",
        "    print(\"‚Ä¢ Pony Diffusion v6 XL\")\n",
        "    print(\"‚Ä¢ NoobAI XL\")\n",
        "    print(\"‚Ä¢ Custom models (enter HuggingFace model ID)\")\n",
        "    print()\n",
        "    print(\"üöÄ INSTRUCTIONS:\")\n",
        "    print(\"1. Upload your SafeTensor (.safetensors) file to Google Colab\")\n",
        "    print(\"2. Get your HuggingFace token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"3. Fill in the parameter forms when you run convert_and_upload_model()\")\n",
        "    print(\"4. The script will automatically:\")\n",
        "    print(\"   ‚Ä¢ Detect the base model (or use your selection)\")\n",
        "    print(\"   ‚Ä¢ Convert to Diffusers format\")\n",
        "    print(\"   ‚Ä¢ Create a detailed model card\")\n",
        "    print(\"   ‚Ä¢ Upload to HuggingFace\")\n",
        "    print()\n",
        "    print(\"üí° PARAMETER EXAMPLES:\")\n",
        "    print(\"CHECKPOINT_PATH: '/content/my-pony-model.safetensors'\")\n",
        "    print(\"HF_TOKEN: 'hf_your_actual_token_here'\")\n",
        "    print(\"REPO_NAME: 'myusername/my-pony-model-diffusers'\")\n",
        "    print(\"BASE_MODEL_TYPE: 'Pony Diffusion v6 XL' (or 'Auto-detect')\")\n",
        "    print()\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Show instructions when the cell is run\n",
        "show_usage_example()\n",
        "\n",
        "print(\"\\nüî• Ready to convert! Run the cell below to start:\")\n",
        "print(\"convert_and_upload_model()\")\n",
        "\n",
        "# Run the conversion (uncomment to execute)\n",
        "convert_and_upload_model()"
      ]
    }
  ]
}